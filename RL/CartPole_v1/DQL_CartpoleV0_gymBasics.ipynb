{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQL_CartpoleV0_gymBasics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_XQ7Sy9ke56"
      },
      "source": [
        "# CartPole-v0:\n",
        "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
        "\n",
        "More description about this game can be found in [open AI gym](https://gym.openai.com/envs/CartPole-v0/) <br>\n",
        "\n",
        "You are required to balance the cartpole for as long as possible, but for simplicity, let's balance it for 500 time steps. So, the maximum length of the episode is 500 timesteps. The episode ends before 500 timesteps if the pole falls off.\n",
        "\n",
        "## Environment\n",
        "\n",
        "**Stae/ Observation**\n",
        "\n",
        "Num| Obsrevation| Minimum Value| Maximmum Value|\n",
        "---|---|---|---|\n",
        "0| Cart Position| -4.8| 4.8|\n",
        "1| Cart Velocity| -Inf | Inf|\n",
        "2| Pole angle|$~-41.8^\\circ$| $~41.8^\\circ$|\n",
        "3| Pole velocity at tip| -Inf| Inf|\n",
        "\n",
        "**Actions**\n",
        "\n",
        "Num| Action|\n",
        "---|---|\n",
        "0| Push cart to the left|\n",
        "1| Push cart to the right|\n",
        "\n",
        "**Reward**\n",
        "\n",
        "Reward is 1 for evary step taken , including terminal state\n",
        "\n",
        "**Initial state**\n",
        "\n",
        "Here, the state is represented by 4 values (Cart Position, Cart Velocity, Pole Angle, Pole Velocity at Tip). All observations are assigned a uniform random value \n",
        "\n",
        "**Episode Termination**\n",
        "1. Pole angle is more than $\\pm{12}^\\circ$\n",
        "2. cart position is more tha $\\pm{2.4}$\n",
        "3. Episode length is greater than 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjsE2gTGzH8b"
      },
      "source": [
        "## Inspect Cartpole-V0 from open AI gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK0Jld5BkQeI"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "#make the environment of a game\n",
        "env= gym.make('CartPole-v0')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3HPa7p5xfIj"
      },
      "source": [
        "Checking environment requirements/ Observation ranges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beYNWZvBtnsS",
        "outputId": "03f2746d-0eb8-4c86-c25a-4d7b3fd1eb22"
      },
      "source": [
        "print(\"Maximum allowed (Cart position, Cart Velocity, Pole Angle, Angular Velocity) \", env.observation_space.high)\n",
        "print(\"Minimum allowed (Cart position, Cart Velocity, Pole Angle, Angular Velocity) \", env.observation_space.low)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum allowed (Cart position, Cart Velocity, Pole Angle, Angular Velocity)  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
            "Minimum allowed (Cart position, Cart Velocity, Pole Angle, Angular Velocity)  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1hcRjJxmf6"
      },
      "source": [
        "Action spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEONMacoxoWq",
        "outputId": "61c0971d-d986-4a53-e8db-a2460c73ae56"
      },
      "source": [
        "print(\"Allowed actions \", env.action_space.n)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allowed actions  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0f_vrXszAZb"
      },
      "source": [
        "A randomly chosen state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0EHGWQqyhA7",
        "outputId": "9614cfed-df23-4e1d-951d-723411c0b429"
      },
      "source": [
        "random_observation= env.reset()\n",
        "print(\"One random observation (Cart position, Cart Velocity, Pole Angle, Angular Velocity) \", random_observation)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One random observation (Cart position, Cart Velocity, Pole Angle, Angular Velocity)  [ 0.00933478  0.04156223 -0.01158811  0.04236648]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPnQrEh0Jza"
      },
      "source": [
        "Take an action in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCnh-Hlo0I-6",
        "outputId": "5859a491-1ad8-4794-8d7c-ed9c064e4f77"
      },
      "source": [
        "action= env.action_space.sample()\n",
        "print(\"Random action \", action)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random action  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHHo8_uE0pKP"
      },
      "source": [
        "Calculate next state, reward, terminal staet achieved ? , info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viDyROVB0aF7",
        "outputId": "0c6d44c2-a450-4019-cb95-2b530466fcf2"
      },
      "source": [
        "observation, reward, done, info= env.step(action)\n",
        "print(\"observation \", observation)\n",
        "print(\"reward \", reward)\n",
        "print(\"Reached terminal state \", done)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation  [ 0.02354544  0.62746262 -0.0268198  -0.84762625]\n",
            "reward  1.0\n",
            "Reached terminal state  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE0nWD7f1VzP"
      },
      "source": [
        "### Run for few episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzmCtIgl1dS0",
        "outputId": "bdb3e070-6618-44ec-ca51-47fa0520f2d5"
      },
      "source": [
        "for episode in range(20):\n",
        "\n",
        "  # Initialize at start of every episode\n",
        "  observation= env.reset() \n",
        "  total_reward = 0\n",
        "\n",
        "  for timesteps in range(200):\n",
        "     # print the movement of the cart pole\n",
        "     #env.render()\n",
        "\n",
        "     # take a random action to generate next step , reward etc\n",
        "     action= env.action_space.sample()\n",
        "     obs, rwd, done, info = env.step(action)\n",
        "\n",
        "     total_reward += reward\n",
        "\n",
        "     if done:\n",
        "       print(\"Episode finished after {0} timesteps with total reward {1}\".format(timesteps+1, total_reward))\n",
        "       break\n",
        "  env.close()\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished after 15 timesteps with total reward 15.0\n",
            "Episode finished after 20 timesteps with total reward 20.0\n",
            "Episode finished after 11 timesteps with total reward 11.0\n",
            "Episode finished after 21 timesteps with total reward 21.0\n",
            "Episode finished after 16 timesteps with total reward 16.0\n",
            "Episode finished after 18 timesteps with total reward 18.0\n",
            "Episode finished after 24 timesteps with total reward 24.0\n",
            "Episode finished after 12 timesteps with total reward 12.0\n",
            "Episode finished after 25 timesteps with total reward 25.0\n",
            "Episode finished after 24 timesteps with total reward 24.0\n",
            "Episode finished after 9 timesteps with total reward 9.0\n",
            "Episode finished after 44 timesteps with total reward 44.0\n",
            "Episode finished after 13 timesteps with total reward 13.0\n",
            "Episode finished after 17 timesteps with total reward 17.0\n",
            "Episode finished after 18 timesteps with total reward 18.0\n",
            "Episode finished after 19 timesteps with total reward 19.0\n",
            "Episode finished after 20 timesteps with total reward 20.0\n",
            "Episode finished after 20 timesteps with total reward 20.0\n",
            "Episode finished after 10 timesteps with total reward 10.0\n",
            "Episode finished after 12 timesteps with total reward 12.0\n"
          ]
        }
      ]
    }
  ]
}