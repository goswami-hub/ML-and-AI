# -*- coding: utf-8 -*-
"""TCGame_Env.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_wBFHxZsSRgj-FxI2rZMeGEXtv1qpsO5
"""

import numpy as np
import random
from itertools import groupby, product

class TicTacToe():
    
  def __init__(self):
    """initialise the board"""
        
    # initialize the state
    self.state= [np.nan for _ in range(9)] # initialises the board position as a list of 9 elements as nan
        
    # All possible numbers alloed in the game , i..e 1 to 9
    self.all_possible_numbers= [i for i in range(1, len(self.state)+1)]
        
    self.reset()   #Reset before every episode
        
  def is_winning(self, curr_state):
    """Takes state as an input and returns whether any row, column or diagonal has winning sum"""
    # To evaluate result the game the row sum , column sum or diagonal sum will be considered
     #  game is won if sum across any direction is equal to 15 , considering all the elements in that direction are non empty
    if (sum(curr_state[0:3:1])==15 or sum(curr_state[3:6:1])==15 or sum(curr_state[6:9:1])==15 or
        sum(curr_state[0:9:3])==15 or sum(curr_state[1:9:3])==15 or sum(curr_state[2:9:3])==15 or
        sum(curr_state[0:9:4])==15 or sum(curr_state[2:8:2])==15):
      return True
    else:
      return False


  def is_terminal(self, curr_state):
    # Terminal state could be winning state or when the board is filled up

    if self.is_winning(curr_state) == True:
      return True, 'Win'

    elif len(self.allowed_positions(curr_state)) ==0:
      return True, 'Tie'

    else:
      return False, 'Resume'
                                                                    
  def allowed_positions(self, curr_state):
    """Takes state as an input and returns all indexes that are blank"""
    return [i for i, val in enumerate(curr_state) if np.isnan(val)]
                                                                                            
  def allowed_values(self, curr_state):
    """Takes the current state as input and returns all possible (unused) values that can be placed on the board"""

    used_values = [val for val in curr_state if not np.isnan(val)]
    agent_values = [val for val in self.all_possible_numbers if val not in used_values and val % 2 !=0]
    env_values = [val for val in self.all_possible_numbers if val not in used_values and val % 2 ==0]

    return (agent_values, env_values)
                                                                                            
  def action_space(self, curr_state):
    """Takes the current state as input and returns all possible actions, i.e, all combinations of allowed positions and allowed values"""

    agent_actions = product(self.allowed_positions(curr_state), self.allowed_values(curr_state)[0])
    env_actions = product(self.allowed_positions(curr_state), self.allowed_values(curr_state)[1])
    return (agent_actions, env_actions)

  def state_transition(self, curr_state, curr_action):
    """Takes current state and action and returns the board position just after agent's move."""
    curr_state[curr_action[0]]= curr_action[1]
    return curr_state
                                                                                            
  def step(self, curr_state, curr_action):
    """Takes current state and action and returns the next state, reward and whether the state is terminal."""
    #Initialize final state to false
    final_state= False
    #Evaluate change in state due to action taken by agent                                                                                    #
    new_state= self.state_transition(curr_state, curr_action)
    #Check if this is a terminal state
    final_state, game_status = self.is_terminal(new_state)
        
    if final_state == True:
      if game_status == 'Win':
        reward = 10
        message='Agent wins'
      else:
        reward = 0
        message = 'No result'
        
    else:
      #Random action by environment                                                                              
      env_pos = np.random.choice(self.allowed_positions(new_state))
      env_value = np.random.choice(self.allowed_values(new_state)[1])
                                                                                        
      #Updated new state after random action by environment
      new_state[env_pos] = env_value
                            
      #Check if this is a terminal state
      final_state, game_status = self.is_terminal(new_state)
                                                                        
      if final_state == True:
        if game_status == 'Win':
          reward = -10
          message= 'Env wins'
        else:
          reward = 0
          message = 'No result'
      else:
        reward= -1
        message = 'Continue'
                                                                                            
    return new_state, reward, final_state, message
                                                                                            
  def reset(self):
    return self.state